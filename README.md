SH_chatbot/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py         # Interface Streamlit (choix du modÃ¨le, historique)
â”‚   â”œâ”€â”€ chatbot.py      # Gestion du chatbot et des requÃªtes vers l'API
â”‚   â”œâ”€â”€ ollama_api.py   # API FastAPI pour la rÃ©cupÃ©ration augmentÃ©e
â”‚   â”œâ”€â”€ faq.py          # Chargement et gestion des FAQs
â”‚   â””â”€â”€ models.py       # Configuration des modÃ¨les (embeddings, RAG)
â”‚
â”œâ”€â”€ requirements.txt    # DÃ©pendances
â”œâ”€â”€ Faq.txt             # Fichier contenant des rÃ©ponses FAQ prÃ©-enregistrÃ©es
â””â”€â”€ README.md           # Documentation du projet

---

### **ğŸ“Œ `app/main.py` - Interface Streamlit avec choix du modÃ¨le**
```python
import streamlit as st
from app.chatbot import chat_with_bot

st.title("SH Chatbot - Recherche AugmentÃ©e")

# Liste des modÃ¨les disponibles
models = ["mistral", "llama2", "gemma"]
selected_model = st.selectbox("Choisissez un modÃ¨le :", models)

# Stocker le modÃ¨le sÃ©lectionnÃ©
st.session_state.model = selected_model

# Initialiser lâ€™historique
if "history" not in st.session_state:
    st.session_state.history = []

# Afficher l'historique des conversations
st.subheader("Historique")
for chat in st.session_state.history:
    st.write(f"**Vous** : {chat['query']}")
    st.write(f"**Bot** : {chat['response']}")
    st.markdown("---")

query = st.text_input("Posez votre question :")

if st.button("Envoyer") and query:
    response = chat_with_bot(query, st.session_state.model)
    
    st.session_state.history.append({"query": query, "response": response})
    st.subheader("RÃ©ponse :")
    st.write(response)

if st.button("RÃ©initialiser l'historique"):
    st.session_state.history = []
    st.experimental_rerun()
```

---

### **ğŸ“Œ `app/chatbot.py` - Gestion des requÃªtes vers l'API**
```python
import requests

def chat_with_bot(query, model="mistral"):
    url = "http://127.0.0.1:8000/rag/"
    response = requests.post(url, params={"query": query, "model": model})

    if response.status_code == 200:
        return response.json()["response"]
    return "Erreur lors de la requÃªte."
```

---

### **ğŸ“Œ `app/ollama_api.py` - API FastAPI pour la rÃ©cupÃ©ration augmentÃ©e**
```python
from fastapi import FastAPI
from app.models import search_with_reranking, generate_response

app = FastAPI()

@app.post("/rag/")
async def rag_endpoint(query: str, model: str = "mistral"):
    top_docs = search_with_reranking(query)
    context = "\n\n".join(top_docs)
    response = generate_response(query, context, model)
    return {"query": query, "response": response, "context": top_docs}
```

---

### **ğŸ“Œ `app/models.py` - Embeddings, RAG et gÃ©nÃ©ration de rÃ©ponse**
```python
from sentence_transformers import SentenceTransformer
import chromadb

# ModÃ¨le d'embedding pour la recherche
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Initialisation de ChromaDB
chroma_client = chromadb.PersistentClient(path="./chroma_db")
collection = chroma_client.get_or_create_collection("rag_docs")

def search_with_reranking(query, top_k=5):
    query_embedding = embedding_model.encode(query).tolist()
    results = collection.query(query_embeddings=[query_embedding], n_results=10)
    return [r["text"] for r in results["metadatas"][0]]

def generate_response(query, context, model="mistral"):
    import ollama
    prompt = f"Contexte : {context}\n\nQuestion : {query}\nRÃ©ponse :"
    return ollama.chat(model=model, messages=[{"role": "user", "content": prompt}])["message"]["content"]
```

---

### **ğŸ“Œ `app/faq.py` - Gestion des FAQs**
```python
def load_faq():
    with open("Faq.txt", "r", encoding="utf-8") as f:
        return [line.strip() for line in f.readlines()]
```

---

### **ğŸ“Œ `requirements.txt` - DÃ©pendances**
```
streamlit
fastapi
uvicorn
sentence-transformers
chromadb
requests
ollama
```

---

## **3ï¸âƒ£ Lancer le projet**
### **1ï¸âƒ£ DÃ©marrer l'API**
```bash
uvicorn app.ollama_api:app --reload
```

### **2ï¸âƒ£ Lancer Streamlit**
```bash
streamlit run app/main.py
```

---

### **ğŸ¯ FonctionnalitÃ©s ajoutÃ©es**
âœ… **Personnalisation du modÃ¨le** (Mistral, Llama2, Gemma, etc.) via un menu dÃ©roulant  
âœ… **Optimisation du RAG** avec **ChromaDB** et **reranking**  
âœ… **Interface utilisateur avec Streamlit**  
âœ… **Historique des conversations**  
âœ… **Architecture bien organisÃ©e**
